{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2beb84e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/fra31/auto-attack\n",
      "  Cloning https://github.com/fra31/auto-attack to /private/var/folders/8d/bk2smpf90bsd0ksdh9yqgct00000gn/T/pip-req-build-zsyglyc3\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/fra31/auto-attack /private/var/folders/8d/bk2smpf90bsd0ksdh9yqgct00000gn/T/pip-req-build-zsyglyc3\n",
      "  Resolved https://github.com/fra31/auto-attack to commit a39220048b3c9f2cca9a4d3a54604793c68eca7e\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: autoattack\n",
      "  Building wheel for autoattack (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for autoattack: filename=autoattack-0.1-py3-none-any.whl size=36229 sha256=e7cdabcfcc6ee689f2c6c2916c70c2ebe7f39a168eda319968c614c1989c9e31\n",
      "  Stored in directory: /private/var/folders/8d/bk2smpf90bsd0ksdh9yqgct00000gn/T/pip-ephem-wheel-cache-vef92x5u/wheels/e1/e8/28/65b2724d4c7740785979eb50bf5e1b3986ead22f6c32a87f8f\n",
      "Successfully built autoattack\n",
      "Installing collected packages: autoattack\n",
      "Successfully installed autoattack-0.1\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/fra31/auto-attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60ecba69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First state of Hadamard matrix\n",
      " [[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 1. -1.  1. -1.  1. -1.  1. -1.  1. -1.  1. -1.  1. -1.  1. -1.]\n",
      " [ 1.  1. -1. -1.  1.  1. -1. -1.  1.  1. -1. -1.  1.  1. -1. -1.]\n",
      " [ 1. -1. -1.  1.  1. -1. -1.  1.  1. -1. -1.  1.  1. -1. -1.  1.]\n",
      " [ 1.  1.  1.  1. -1. -1. -1. -1.  1.  1.  1.  1. -1. -1. -1. -1.]\n",
      " [ 1. -1.  1. -1. -1.  1. -1.  1.  1. -1.  1. -1. -1.  1. -1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1.  1.  1. -1. -1. -1. -1.  1.  1.]\n",
      " [ 1. -1. -1.  1. -1.  1.  1. -1.  1. -1. -1.  1. -1.  1.  1. -1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [ 1. -1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1.  1. -1.  1.]]\n",
      "Minimum Hamming distance : 7\n",
      "Distance matrix\n",
      " [[0 8 8 8 8 8 8 8 8 8]\n",
      " [8 0 8 8 8 8 8 8 8 8]\n",
      " [8 8 0 8 8 8 8 8 8 8]\n",
      " [8 8 8 0 8 8 8 8 8 8]\n",
      " [8 8 8 8 0 8 8 8 8 8]\n",
      " [8 8 8 8 8 0 8 8 8 8]\n",
      " [8 8 8 8 8 8 0 8 8 8]\n",
      " [8 8 8 8 8 8 8 0 8 8]\n",
      " [8 8 8 8 8 8 8 8 0 8]\n",
      " [8 8 8 8 8 8 8 8 8 0]]\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import Models\n",
    "from MyTorch import Classifier\n",
    "import torch \n",
    "from matplotlib import pyplot as plt \n",
    "import save_load_interface\n",
    "import Metrics\n",
    "import scipy.linalg\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "code_length=16\n",
    "num_codes=10\n",
    "#seed=59\n",
    "\n",
    "M = scipy.linalg.hadamard(code_length).astype(np.float32) # Hadamard matrix generate processing\n",
    "M = M[0:num_codes]\n",
    "print(\"First state of Hadamard matrix\\n\", M)\n",
    "\n",
    "min_hamming = int(math.pow(2, math.sqrt(code_length)-1) -1)\n",
    "print(\"Minimum Hamming distance :\", min_hamming)\n",
    "\n",
    "# distance_matrix 만드는 함수야\n",
    "def generating_distance_matrix(codebook, num_classes, code_length):\n",
    "    distance_matrix = np.zeros((num_codes, num_codes), dtype=int)\n",
    "    for i in range(num_codes):\n",
    "        for j in range(num_codes):\n",
    "            distance_matrix[i, j] = np.sum(codebook[i] != codebook[j])\n",
    "    print(\"Distance matrix\\n\", distance_matrix)\n",
    "    \n",
    "generating_distance_matrix(M, num_codes, code_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11e25032",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/jang-ulam/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/8d/bk2smpf90bsd0ksdh9yqgct00000gn/T/ipykernel_16140/3549741116.py\", line 25, in <module>\n",
      "    min_ham = min_hamming_distance_row(temp_M)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/8d/bk2smpf90bsd0ksdh9yqgct00000gn/T/ipykernel_16140/2322254734.py\", line 16, in min_hamming_distance_row\n",
      "    dist = hamming_distance(W[pair[0]], W[pair[1]])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/8d/bk2smpf90bsd0ksdh9yqgct00000gn/T/ipykernel_16140/2322254734.py\", line -1, in hamming_distance\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jang-ulam/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jang-ulam/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jang-ulam/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jang-ulam/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jang-ulam/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jang-ulam/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jang-ulam/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/jang-ulam/anaconda3/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/jang-ulam/anaconda3/lib/python3.11/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jang-ulam/anaconda3/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/jang-ulam/anaconda3/lib/python3.11/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jang-ulam/anaconda3/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/jang-ulam/anaconda3/lib/python3.11/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"/Users/jang-ulam/anaconda3/lib/python3.11/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "## optimizing codebook by bits flipping\n",
    "\n",
    "dlist = []\n",
    "\n",
    "cnt=0\n",
    "\n",
    "temp = M.copy()\n",
    "max = 0\n",
    "\n",
    "\n",
    "ok = False \n",
    "while(not ok):\n",
    "    if cnt >= 1000:\n",
    "        break\n",
    "    print(cnt)\n",
    "\n",
    "    t = True\n",
    "    while(t):\n",
    "        row = np.random.randint(0, num_codes)\n",
    "        idx = np.random.randint(0, code_length)\n",
    "        \n",
    "        temp_M = temp.copy()\n",
    "        temp_M[row][idx] *= -1\n",
    "        \n",
    "        min_ham = min_hamming_distance_row(temp_M)\n",
    "        max_ham = max_hamming_distance_row(temp_M)\n",
    "\n",
    "        if min_ham >= min_hamming and max_ham > max:\n",
    "            print(\"max_ham : \", max)\n",
    "            max = max_ham\n",
    "            max_M = temp_M\n",
    "            temp = temp_M\n",
    "            t = False\n",
    "            cnt+=1\n",
    "        else:\n",
    "            cnt+=1\n",
    "            print(\"Hamming distance 증가 실패\")\n",
    "            \n",
    "dlist.append(max_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "619842b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "[[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 1. -1.  1. -1.  1. -1.  1. -1.  1. -1.  1. -1.  1. -1.  1. -1.]\n",
      " [ 1.  1. -1. -1.  1.  1. -1. -1.  1.  1. -1. -1.  1.  1. -1. -1.]\n",
      " [ 1. -1. -1.  1.  1. -1. -1.  1.  1. -1. -1.  1.  1. -1. -1.  1.]\n",
      " [ 1.  1.  1.  1. -1. -1. -1. -1.  1.  1.  1.  1. -1. -1. -1. -1.]\n",
      " [-1.  1.  1. -1.  1.  1. -1.  1. -1.  1.  1. -1.  1. -1. -1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1.  1.  1. -1. -1. -1. -1.  1.  1.]\n",
      " [ 1. -1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1. -1.  1.  1. -1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [ 1. -1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1.  1. -1.  1.]]\n",
      "7\n",
      "Min hamming distance Row: 7\n",
      "Max hamming distance Row: 16\n",
      "Distance matrix\n",
      " [[ 0  8  8  8  8  7  8  9  8  8]\n",
      " [ 8  0  8  8  8  9  8  7  8  8]\n",
      " [ 8  8  0  8  8  7  8  9  8  8]\n",
      " [ 8  8  8  0  8  9  8  7  8  8]\n",
      " [ 8  8  8  8  0  9  8  7  8  8]\n",
      " [ 7  9  7  9  9  0  9 16  7  9]\n",
      " [ 8  8  8  8  8  9  0  7  8  8]\n",
      " [ 9  7  9  7  7 16  7  0  9  7]\n",
      " [ 8  8  8  8  8  7  8  9  0  8]\n",
      " [ 8  8  8  8  8  9  8  7  8  0]]\n"
     ]
    }
   ],
   "source": [
    "print(max_ham)\n",
    "print(max_M)\n",
    "print(min_hamming)\n",
    "min_ = min_hamming_distance_row(max_M) \n",
    "print(\"Min hamming distance Row: {}\".format(min_))\n",
    "\n",
    "max_ = max_hamming_distance_row(max_M) \n",
    "print(\"Max hamming distance Row: {}\".format(max_))\n",
    "\n",
    "W = np.load('codewords/codeword_' + str(16) + '_bit.npy', allow_pickle=True) \n",
    "W[W==0] = -1   # scale codes to [-1, 1] range instead of [0,1] and get first code of list\n",
    "\n",
    "for i in range(len(dlist)):\n",
    "    generating_distance_matrix(max_M, num_codes, code_length)\n",
    "    \n",
    "generating_distance_matrix(max_M, num_codes, code_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b4490a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4677904\n",
      "4677904\n",
      "\n",
      "Training ECOC Model\n",
      "Dataset: CIFAR10\n",
      "Batch size: 100, Nb of epochs: 1000, learning rate: 0.0001\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "epoch :  0\n",
      "batch :  0\n",
      "batch :  1\n",
      "batch :  2\n",
      "batch :  3\n",
      "batch :  4\n",
      "batch :  5\n",
      "batch :  6\n",
      "batch :  7\n",
      "batch :  8\n",
      "batch :  9\n",
      "batch :  10\n",
      "batch :  11\n",
      "batch :  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Main script for training ECOC ensembles\n",
    "\n",
    "import numpy as np \n",
    "import torch.nn\n",
    "import torch\n",
    "import Models \n",
    "from Loss_Function import hinge_vec\n",
    "import save_load_interface\n",
    "from MyTorch_ecoc import Classifier_ecoc as Classifier_ecoc\n",
    "\n",
    "# parameters for PyTorch model type and its training\n",
    "dataset_name = \"CIFAR10\" #\"CIFAR10\"  or \"Fashion-MNIST\"\n",
    "batch_size = 100\n",
    "nb_epoch = 1000\n",
    "lr = 0.0001 \n",
    "seed = 42       \n",
    "path_checkpoint = \"ecoc_16_1/\"  # were the checkpoints are saved\n",
    "save_iter = 10                         # interval between checkpoints being saved\n",
    "nb_independent_models = 16            # Number of shared feature extractors (for independent models, 16, 32 and 16 since they are independent)\n",
    "nb_bits_per_model = 1\n",
    "filters = [16, 32, 64] #[32, 64, 128] # params A, B, C\n",
    "filter2 = [64]            # param D\n",
    "RegAdvt = False          # if want to perform RegAdvt\n",
    "\n",
    "# Set to true to load a model and resume its training\n",
    "load_model = False\n",
    "load_model_filename = \"checkpoints/cifar10/current_experimentation/epoch_810.pth\"      \n",
    "\n",
    "# load W from files\n",
    "load_codebook = False\n",
    "if load_codebook:\n",
    "    W = np.load('codewords/codeword_' + str(nb_bits_per_model*nb_independent_models) + '_bit.npy', allow_pickle=True) \n",
    "    W[W==0] = -1   # codewords need to be [-1, 1] instead of [0, 1]\n",
    "else:\n",
    "    W = max_M\n",
    "    \n",
    "# Init ECOC Model\n",
    "model = Models.ecoc_ensemble_no_bn(W, nb_independent_models, filters, filter2, nb_independent_models*nb_bits_per_model, dataset_name) \n",
    "\n",
    "# ECOC Resnet Models \n",
    "model = Models.ResNetECOC(W, nb_independent_models, nb_independent_models) \n",
    "\n",
    "total_params = sum(param.numel() for param in model.parameters())\n",
    "print(total_params)\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(trainable_params)\n",
    "quit()\n",
    "\n",
    "# Training params \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = hinge_vec()    # hinge loss for ECOC ensembles\n",
    "\n",
    "# Set dataset dependent params \n",
    "if dataset_name == \"Fashion-MNIST\": \n",
    "    input_shape = (1, 28, 28) \n",
    "    nb_classes = 10\n",
    "\n",
    "elif dataset_name == \"CIFAR10\": \n",
    "    input_shape = (3, 32, 32)\n",
    "    nb_classes = 10                             \n",
    "\n",
    "print(\"\\nTraining ECOC Model\" ) \n",
    "print(\"Dataset: {}\".format(dataset_name))\n",
    "print(\"Batch size: {}, Nb of epochs: {}, learning rate: {}\".format(batch_size, nb_epoch, lr))\n",
    "\n",
    "#check if cuda is available. Training is much faster on the GPU. \n",
    "device = torch.device(\"cuda\" if(torch.cuda.is_available()) else \"cpu\") \n",
    "\n",
    "if load_model: \n",
    "    checkpoint = save_load_interface.load_checkpoint(load_model_filename)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    model.to(device)  # need to do it here otherwise the optimizer parameters are on cpu instead of device (or cuda if gpu available)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    for g in optimizer.param_groups:  \n",
    "        g['lr']= lr\n",
    "    criterion = checkpoint['criterion']\n",
    "    classifier = Classifier_ecoc(model, W, dataset_name, device, nb_classes, batch_size, input_shape, optimizer, criterion, seed)\n",
    "    classifier.set_history(checkpoint['train_accuracy'], checkpoint['validation_accuracy'], checkpoint['train_loss'], checkpoint['val_loss'], checkpoint['euclidean_dist'], checkpoint['hamming_dist'], checkpoint['hamming_dist_val'], checkpoint['euclidean_dist_val'])\n",
    "else:      \n",
    "    classifier = Classifier_ecoc(model, W, dataset_name, device, nb_classes, batch_size, input_shape, optimizer, criterion, seed) \n",
    "\n",
    "# Train ECOC model\n",
    "classifier.fit(nb_epoch, 0.98, save_iter, path=path_checkpoint, advt=RegAdvt) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "a7b80921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Adversarial Attack\n",
      "CUDA available: False\n",
      "\n",
      "Dataset: CIFAR10\n",
      "Attack Type: PGD, Epsilon: 0.031, Epsilon step: 0.010333333333333333, Max Iter: 100, Nb Images: 2000\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to datasets/cifar10/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████| 170498071/170498071 [00:38<00:00, 4430891.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/cifar10/cifar-10-python.tar.gz to datasets/cifar10\n",
      "Files already downloaded and verified\n",
      "Error: the file name for the checkpoint doesn't exists\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[303], line 108\u001b[0m\n\u001b[1;32m    103\u001b[0m         np\u001b[38;5;241m.\u001b[39msavez(path_metrics, true_labels\u001b[38;5;241m=\u001b[39mlabels_list, pred_labels_clean\u001b[38;5;241m=\u001b[39mpredictions_clean, predicted_labels\u001b[38;5;241m=\u001b[39mpred_labels_list, true_codes\u001b[38;5;241m=\u001b[39mtrue_codes_list, predicted_codes\u001b[38;5;241m=\u001b[39minferred_codes_list, real_predicted_codes\u001b[38;5;241m=\u001b[39mreal_inferred_codes_list)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m: \n\u001b[0;32m--> 108\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[303], line 64\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Lad the ECOC model from files\u001b[39;00m\n\u001b[1;32m     63\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m save_load_interface\u001b[38;5;241m.\u001b[39mload_checkpoint(root)\n\u001b[0;32m---> 64\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     65\u001b[0m full_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSequential(model, Models\u001b[38;5;241m.\u001b[39mecoc_decoder(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(W)\u001b[38;5;241m.\u001b[39mto(device), no_tanh\u001b[38;5;241m=\u001b[39mplus))  \u001b[38;5;66;03m# sequence of ecoc model and decoder to output the probabilities of each class \u001b[39;00m\n\u001b[1;32m     66\u001b[0m full_model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'bool' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# main script for attacking ECOC ensembles \n",
    "\n",
    "import Models\n",
    "import AdversarialAttackCleverHans\n",
    "import torch\n",
    "import save_load_interface\n",
    "import numpy as np\n",
    "import Metrics\n",
    "import Dataset\n",
    "from Loss_Function import cw_loss\n",
    "\n",
    "# parameters for PyTorch model type and its training\n",
    "dataset_name = \"CIFAR10\" #\"Fashion-MNIST\"  #Fashion-MNIST or CIFAR10 (must be written this way)\n",
    "batch_size = 100  \n",
    "seed = 42  # for making sure same images are used every time\n",
    "filters = [32, 64, 128] # params A, B, C\n",
    "filter2 = [16]            # param D\n",
    "root = 'models_final/CIFAR10/AdvT/IndAdvt_2iter/indAdvt_2iter.pth' # for SIMPLE model, provide path of simple_1 model. For ENSEMBLE models, provide path of folder where all SIMPLE models are\n",
    "\n",
    "# params for performing attacks \n",
    "attack_type = \"PGD\"      # can be 'FGSM' or 'PDG' or 'CWL2' or 'auto' For CWL2, need to save the perturbations and run the script \"cw_bound.py\" to get the accuracy with a L2 bound. Otherwise, the perturbations generated in this script are not bounded. \n",
    "norm = np.inf             # norm of the attack (np.inf or 2)\n",
    "max_iter = 100            # max iteration for PGD attack \n",
    "es = True                 # True: PGD^es, False: PGD\n",
    "plus = False              # adaptation for removing operations that could cause gradient masking (i.e., softmax in soft voter of ensemble baseline models)\n",
    "epsilon =  0.031          # bound the attack norm \n",
    "eps_step = 1/3 * epsilon  # epsilon step for PGD attack\n",
    "loss = cw_loss            # torch.nn.CrossEntropyLoss()   # loss function for PGD attack \n",
    "nb_images = 2000            \n",
    "\n",
    "# Parameters\n",
    "seed = 42   # to make sure we use the same images everytime \n",
    "batch_size = 1 #100\n",
    "nb_independent_models = 16\n",
    "nb_bits_per_model = 1\n",
    "save_adversarial_examples = False\n",
    "\n",
    "# Params for loading model and saving attack images and metrics \n",
    "path_adversaries = \"current_experimentation/PGD_0_031_tanh.pth\"            # where the adversarial images will be saved                             \n",
    "path_metrics = \"checkpoints/cifar10/current_experimentation/metrics.npz\"   # where the metrics (hamm dist, codes, etc) will be saved\n",
    "\n",
    "W = max_M\n",
    "\n",
    "# Model architecture details for ecoc models                                                             \n",
    "model = Models.ecoc_ensemble_no_bn(W, nb_independent_models, filters, filter2, nb_independent_models*nb_bits_per_model, dataset_name) \n",
    "\n",
    "\n",
    "def main(): \n",
    "    print(\"\\nGenerating Adversarial Attack\" ) \n",
    "    # check if cuda available \n",
    "    print(\"CUDA available: {}\\n\".format(torch.cuda.is_available()))\n",
    "    device = torch.device(\"cuda\" if(torch.cuda.is_available()) else \"cpu\") \n",
    "\n",
    "    # print experimentation details \n",
    "    print(\"Dataset: {}\".format(dataset_name))\n",
    "    print(\"Attack Type: {}, Epsilon: {}, Epsilon step: {}, Max Iter: {}, Nb Images: {}\".format(attack_type, epsilon, eps_step, max_iter, nb_images))\n",
    "\n",
    "\n",
    "    # get testing data\n",
    "    _, test_dataset = Dataset.getDataset(dataset_name)\n",
    "    test_dataloader = Dataset.get_dataLoader(test_dataset, batch_size)\n",
    " \n",
    "    # Load the ECOC model from files\n",
    "    checkpoint = save_load_interface.load_checkpoint(root)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    \n",
    "    full_model = torch.nn.Sequential(model, Models.ecoc_decoder(torch.from_numpy(W).to(device), no_tanh=plus))  # sequence of ecoc model and decoder to output the probabilities of each class \n",
    "    full_model.to(device)\n",
    "    full_model.eval()\n",
    "\n",
    "    ### Adversarial attack (generate adversarial examples)\n",
    "    test_data = Dataset.get_test_subset(test_dataset, batch_size, nb_images, seed=seed)\n",
    "    if attack_type == 'FGSM':\n",
    "        data_perturbed = AdversarialAttackCleverHans.FGSM(full_model, batch_size, test_data, epsilon, norm)             \n",
    "    elif attack_type == 'PGD':\n",
    "        data_perturbed = AdversarialAttackCleverHans.PGD(full_model, batch_size, test_data, epsilon, eps_step, max_iter, norm, loss, early_stop=True)\n",
    "    elif attack_type == 'CW':\n",
    "        data_perturbed = AdversarialAttackCleverHans.CW_L2(full_model, batch_size, test_data, max_iter=max_iter)\n",
    "    elif attack_type == 'auto': \n",
    "        data_perturbed = AdversarialAttackCleverHans.autoAttack(full_model, batch_size, test_data, epsilon, norm)\n",
    "\n",
    "\n",
    "    # compute accuracy before and after the attack\n",
    "    predictions_clean, true_labels_clean = Metrics.predict_eval(full_model, test_data, device) \n",
    "    predictions_perturbed, true_labels_perturbed = Metrics.predict_eval(full_model, data_perturbed, device)\n",
    "    accuracy_clean = Metrics.score(predictions_clean, true_labels_clean) \n",
    "    accuracy_perturbed = Metrics.score(predictions_perturbed, true_labels_perturbed)    \n",
    "    asr = Metrics.success_rate(predictions_clean, true_labels_clean, predictions_perturbed)\n",
    "    print(\"Test accuracy before the attack: {}\".format(accuracy_clean))\n",
    "    print(\"Accuracy of classifier against attack: {}\".format(accuracy_perturbed))\n",
    "    print(\"Attack success rate: {}\".format(asr))\n",
    "\n",
    "  \n",
    "    # Save adversarial examples\n",
    "    if save_adversarial_examples: \n",
    "        save_load_interface.save_adversarial_examples(path_adversaries, data_perturbed)\n",
    "    \n",
    "        # save a bunch of metrics to file for later analysis\n",
    "        labels_list, pred_labels_list, true_codes_list, inferred_codes_list, real_inferred_codes_list = Metrics.assess_codes(full_model, data_perturbed, torch.from_numpy(W).to(device), device) \n",
    "        labels_list = labels_list.numpy()\n",
    "        pred_labels_list = pred_labels_list.numpy()\n",
    "        true_codes_list = true_codes_list.numpy()\n",
    "        inferred_codes_list = inferred_codes_list.numpy()\n",
    "        real_inferred_codes_list = real_inferred_codes_list.numpy()\n",
    "        np.savez(path_metrics, true_labels=labels_list, pred_labels_clean=predictions_clean, predicted_labels=pred_labels_list, true_codes=true_codes_list, predicted_codes=inferred_codes_list, real_predicted_codes=real_inferred_codes_list)\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    main() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "a22a32a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.]\n",
      " [ 1.  1. -1. -1.  1. -1. -1. -1.  1.  1. -1. -1.  1.  1. -1. -1.]\n",
      " [ 1. -1. -1.  1.  1. -1. -1.  1.  1. -1. -1.  1.  1. -1. -1.  1.]\n",
      " [ 1.  1.  1.  1. -1. -1. -1. -1.  1.  1.  1.  1. -1. -1. -1. -1.]\n",
      " [ 1. -1.  1. -1. -1.  1. -1.  1.  1. -1.  1. -1. -1.  1. -1.  1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1.  1.  1. -1. -1. -1. -1.  1.  1.]\n",
      " [ 1. -1. -1.  1. -1.  1.  1. -1.  1. -1. -1.  1. -1.  1.  1. -1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      " [ 1. -1.  1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1.  1. -1.  1.]]\n",
      "1 0\n",
      "1 2\n",
      "1 4\n",
      "1 6\n",
      "1 8\n",
      "1 11\n",
      "1 12\n",
      "1 13\n",
      "1 14\n",
      "2 5\n",
      "16\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(max_M)\n",
    "for i in range(num_codes):\n",
    "    for j in range(code_length):\n",
    "        if M[i][j] != max_M[i][j] :\n",
    "            print(i, j)\n",
    "\n",
    "print(min_hamming_distance_row_complement(max_M))\n",
    "print(min_hamming_distance_column(max_M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ece9ba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute min hamming distance between all possible pairs of codewords from the W matrix \n",
    "def min_hamming_distance_row(W):\n",
    "    '''\n",
    "    Compute minimum hamming distance between rows of a matrix of codewords\n",
    "\n",
    "            parameters: W (numpy 2d array): matrix of codewords\n",
    "\n",
    "            Returns: min hamming_distance (int) : minimum hamming distance \n",
    "    ''' \n",
    "    nb_rows = W.shape[0]\n",
    "    nb_columns = W.shape[1]\n",
    "    pairs = [(i, j) for i in range(nb_rows) for j in range(i+1, nb_rows)] \n",
    "    min = W.shape[1]\n",
    "    dlist = []\n",
    "    for pair in pairs:\n",
    "        dist = hamming_distance(W[pair[0]], W[pair[1]])\n",
    "        dlist.append(dist)\n",
    "        if dist < min: \n",
    "            min = dist\n",
    "    return min\n",
    "\n",
    "def max_hamming_distance_row(W):\n",
    "    '''\n",
    "    Compute maximum hamming distance between rows of a matrix of codewords\n",
    "\n",
    "            parameters: W (numpy 2d array): matrix of codewords\n",
    "\n",
    "            Returns: min hamming_distance (int) : minimum hamming distance \n",
    "    ''' \n",
    "    nb_rows = W.shape[0]\n",
    "    nb_columns = W.shape[1]\n",
    "    pairs = [(i, j) for i in range(nb_rows) for j in range(i+1, nb_rows)] \n",
    "    max = 0\n",
    "    dlist = []\n",
    "    for pair in pairs:\n",
    "        dist = hamming_distance(W[pair[0]], W[pair[1]])\n",
    "        dlist.append(dist)\n",
    "        if dist > max: \n",
    "            max = dist\n",
    "    return max\n",
    "\n",
    "def min_hamming_distance_row_complement(W):\n",
    "    '''\n",
    "    Compute minimum hamming distance between rows of a matrix of codewords\n",
    "\n",
    "            parameters: W (numpy 2d array): matrix of codewords\n",
    "\n",
    "            Returns: min hamming_distance (int) : minimum hamming distance \n",
    "    ''' \n",
    "    nb_rows = W.shape[0]\n",
    "    nb_columns = W.shape[1]\n",
    "    pairs = [(i, j) for i in range(nb_rows) for j in range(i+1, nb_rows)] \n",
    "    min = W.shape[1]\n",
    "    dlist = []\n",
    "    for pair in pairs:\n",
    "        dist = hamming_distance(1 - W[pair[0]], W[pair[1]])\n",
    "        dlist.append(dist)\n",
    "        if dist < min: \n",
    "            min = dist\n",
    "    for pair in pairs:\n",
    "        dist = hamming_distance(W[pair[0]], 1 - W[pair[1]])\n",
    "        dlist.append(dist)\n",
    "        if dist < min: \n",
    "            min = dist\n",
    "    return min\n",
    "\n",
    "def min_hamming_distance_column(W):\n",
    "    '''\n",
    "    Compute minimum hamming distance betweem rows of a matrix of codeword \n",
    "\n",
    "            parameters: W (numpy 2d array): matrix of codewords\n",
    "\n",
    "            Returns: min hamming_distance (int) : minimum hamming distance \n",
    "    ''' \n",
    "    nb_rows = W.shape[0]\n",
    "    nb_columns = W.shape[1]\n",
    "\n",
    "    pairs = [(i, j) for i in range(nb_columns) for j in range(i+1, nb_columns)] \n",
    "    min = W.shape[0]\n",
    "    dlist = []\n",
    "    for pair in pairs: \n",
    "        dist = hamming_distance(W[:, pair[0]], W[:, pair[1]])\n",
    "        dlist.append(dist)\n",
    "        if dist < min: \n",
    "            min = dist \n",
    "    count = 0 \n",
    "    tot = 0\n",
    "    nb_zero = 0\n",
    "    for item in dlist: \n",
    "        count+= 1 \n",
    "        tot += item\n",
    "        if item == 0 : \n",
    "            nb_zero += 1 \n",
    "\n",
    "    return min \n",
    "\n",
    "\n",
    "def ham_dist_col(W, val):\n",
    "    '''\n",
    "    Compute minimum hamming distance betweem rows of a matrix of codeword \n",
    "\n",
    "            parameters: W (numpy 2d array): matrix of codewords\n",
    "\n",
    "            Returns: min hamming_distance (int) : minimum hamming distance \n",
    "    ''' \n",
    "    flag = 0\n",
    "    nb_columns = W.shape[1]\n",
    "    pairs = [(i, j) for i in range(nb_columns) for j in range(i+1, nb_columns)] \n",
    "    dlist = []\n",
    "    for pair in pairs: \n",
    "        dist = hamming_distance(W[:, pair[0]], W[:, pair[1]])\n",
    "        print(dist)\n",
    "        dlist.append(dist)\n",
    "        if dist > val: \n",
    "            flag = 1\n",
    "    return flag\n",
    "    \n",
    "def min_hamming_distance_column_complement(W):\n",
    "    '''\n",
    "    Compute minimum hamming distance betweem rows of a matrix of codeword \n",
    "\n",
    "            parameters: W (numpy 2d array): matrix of codewords\n",
    "\n",
    "            Returns: min hamming_distance (int) : minimum hamming distance \n",
    "    ''' \n",
    "    nb_rows = W.shape[0]\n",
    "    nb_columns = W.shape[1]\n",
    "\n",
    "    pairs = [(i, j) for i in range(nb_columns) for j in range(i+1, nb_columns)] \n",
    "    min = W.shape[0]\n",
    "    dlist = []\n",
    "    for pair in pairs: \n",
    "        dist = hamming_distance(1 - W[:, pair[0]], W[:, pair[1]])\n",
    "        dlist.append(dist)\n",
    "        if dist < min: \n",
    "            min = dist \n",
    "    count = 0 \n",
    "    tot = 0\n",
    "    for item in dlist: \n",
    "        count+= 1 \n",
    "        tot += item\n",
    "\n",
    "    for pair in pairs: \n",
    "        dist = hamming_distance(W[:, pair[0]], 1 - W[:, pair[1]])\n",
    "        dlist.append(dist)\n",
    "        if dist < min: \n",
    "            min = dist \n",
    "    count = 0 \n",
    "    tot = 0\n",
    "    for item in dlist: \n",
    "        count+= 1\n",
    "        tot += item\n",
    "\n",
    "    return min\n",
    "\n",
    "# Function to compute hamming distance between two codewords\n",
    "def hamming_distance(a, b): \n",
    "    '''\n",
    "    Compute hamming distance between a and b \n",
    "\n",
    "            parameters: a, b (numpy arrays): input vectors (mush have the same length)\n",
    "\n",
    "            Returns: hamming_distance (int) : hamming distance \n",
    "    '''\n",
    "    count = 0\n",
    "    if a.shape == b.shape: \n",
    "        for i, elem in enumerate(a): \n",
    "            if elem != b[i]: \n",
    "                count += 1             \n",
    "    else: \n",
    "        print(\"Cannot compute hamming distance between two vectors of different sizes\")\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9edf2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
